import time
from datetime import UTC, datetime, timedelta
from pathlib import Path
from typing import Any

from sqlalchemy.orm import Session

from app.models import (
    CalendarEvent,
    CalendarStatus,
    DashboardStatistic,
    Episode,
    JellyseerrRequest,
    LibraryItem,
    LibraryItemTorrent,
    MediaType,
    RequestPriority,
    RequestStatus,
    Season,
    ServiceConfiguration,
    ServiceType,
    StatType,
    SyncMetadata,
    SyncStatus,
)
from app.services import JellyfinConnector, JellyseerrConnector, RadarrConnector, SonarrConnector


class SyncService:
    """Service de synchronisation des donn√©es depuis les APIs externes"""

    def __init__(self, db: Session):
        self.db = db

    def get_active_service(self, service_type: ServiceType) -> ServiceConfiguration:
        """R√©cup√©rer la configuration d'un service actif"""
        return (
            self.db.query(ServiceConfiguration)
            .filter(ServiceConfiguration.service_name == service_type, ServiceConfiguration.is_active.is_(True))
            .first()
        )

    def _upsert_torrent(
        self,
        library_item_id: str,
        torrent_hash: str,
        episode_id: int | None = None,
        season_number: int | None = None,
        is_season_pack: bool = False,
    ):
        """Insert a torrent row if it doesn't already exist for this item+hash"""
        existing = (
            self.db.query(LibraryItemTorrent)
            .filter(
                LibraryItemTorrent.library_item_id == library_item_id,
                LibraryItemTorrent.torrent_hash == torrent_hash,
            )
            .first()
        )
        if not existing:
            self.db.add(
                LibraryItemTorrent(
                    library_item_id=library_item_id,
                    torrent_hash=torrent_hash,
                    episode_id=episode_id,
                    season_number=season_number,
                    is_season_pack=is_season_pack,
                )
            )

    def update_sync_metadata(
        self, service_type: ServiceType, status: SyncStatus, records: int = 0, duration_ms: int = 0, error: str = None
    ):
        """Mettre √† jour les m√©tadonn√©es de sync"""
        sync_meta = self.db.query(SyncMetadata).filter(SyncMetadata.service_name == service_type).first()

        if not sync_meta:
            sync_meta = SyncMetadata(service_name=service_type)
            self.db.add(sync_meta)

        sync_meta.last_sync_time = datetime.now(UTC)
        sync_meta.sync_status = status
        sync_meta.records_synced = records
        sync_meta.sync_duration_ms = duration_ms
        sync_meta.error_message = error
        sync_meta.next_sync_time = datetime.now(UTC) + timedelta(minutes=15)

        self.db.commit()

    async def sync_monitored_items(self) -> dict[str, Any]:
        """
        Synchroniser les statistiques des items monitor√©s (Radarr + Sonarr)
        """
        print("üìä Synchronisation des items monitor√©s...")

        try:
            # Initialiser les totaux
            total_monitored = 0
            total_unmonitored = 0
            downloading = 0
            downloaded = 0
            missing = 0
            queued = 0
            unreleased = 0

            # === RADARR ===
            radarr_service = self.get_active_service(ServiceType.RADARR)
            if radarr_service:
                radarr_connector = RadarrConnector(
                    base_url=radarr_service.url, api_key=radarr_service.api_key, port=radarr_service.port
                )

                try:
                    radarr_stats = await radarr_connector.get_statistics()

                    # Ajouter les stats Radarr
                    total_monitored += radarr_stats.get("monitored_movies", 0)
                    total_unmonitored += radarr_stats.get("total_movies", 0) - radarr_stats.get("monitored_movies", 0)
                    downloaded += radarr_stats.get("downloaded_movies", 0)
                    missing += radarr_stats.get("missing_movies", 0)

                    print(f"  üìΩÔ∏è  Radarr: {radarr_stats.get('monitored_movies', 0)} monitor√©s")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Erreur stats Radarr: {e}")
                finally:
                    await radarr_connector.close()

            # === SONARR ===
            sonarr_service = self.get_active_service(ServiceType.SONARR)
            if sonarr_service:
                sonarr_connector = SonarrConnector(
                    base_url=sonarr_service.url, api_key=sonarr_service.api_key, port=sonarr_service.port
                )

                try:
                    sonarr_stats = await sonarr_connector.get_statistics()

                    # Ajouter les stats Sonarr
                    total_monitored += sonarr_stats.get("monitored_series", 0)
                    total_unmonitored += sonarr_stats.get("total_series", 0) - sonarr_stats.get("monitored_series", 0)
                    downloaded += sonarr_stats.get("downloaded_episodes", 0)
                    missing += sonarr_stats.get("missing_episodes", 0)

                    print(f"  üì∫ Sonarr: {sonarr_stats.get('monitored_series', 0)} monitor√©s")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Erreur stats Sonarr: {e}")
                finally:
                    await sonarr_connector.close()

            # Mettre √† jour ou cr√©er la statistique MONITORED_ITEMS
            monitored_stat = (
                self.db.query(DashboardStatistic)
                .filter(DashboardStatistic.stat_type == StatType.MONITORED_ITEMS)
                .first()
            )

            if not monitored_stat:
                monitored_stat = DashboardStatistic(stat_type=StatType.MONITORED_ITEMS)
                self.db.add(monitored_stat)

            # Total = items monitor√©s
            monitored_stat.total_count = total_monitored
            monitored_stat.details = {
                "monitored": total_monitored,
                "unmonitored": total_unmonitored,
                "downloading": downloading,
                "downloaded": downloaded,
                "missing": missing,
                "queued": queued,
                "unreleased": unreleased,
            }
            monitored_stat.last_synced = datetime.now(UTC)

            self.db.commit()

            print(f"‚úÖ Monitored Items: {total_monitored} monitor√©s, {missing} manquants")
            return {
                "success": True,
                "monitored": total_monitored,
                "unmonitored": total_unmonitored,
                "details": monitored_stat.details,
            }

        except Exception as e:
            print(f"‚ùå Erreur sync monitored items: {e}")
            return {"success": False, "error": str(e)}

    async def sync_radarr(self) -> dict[str, Any]:
        """Synchroniser les donn√©es Radarr"""
        print("üé¨ Synchronisation Radarr...")
        start_time = time.time()

        service = self.get_active_service(ServiceType.RADARR)
        if not service:
            print("‚ö†Ô∏è Service Radarr non configur√©")
            return {"success": False, "message": "Service non configur√©"}

        connector = RadarrConnector(base_url=service.url, api_key=service.api_key, port=service.port)

        try:
            # R√©cup√©rer tous les films
            all_movies = await connector.get_movies()

            # Profils de qualit√© {id: name}
            quality_profiles = await connector.get_quality_profiles()

            # R√©cup√©rer la map movieId -> torrent_hash
            movie_hash_map = await connector.get_movie_history_map()
            print(f"üì• {len(movie_hash_map)} hash de torrents r√©cup√©r√©s depuis Radarr")
            print(f"üìΩÔ∏è  {len(all_movies)} films trouv√©s dans Radarr")

            # Ajouter √† la DB (√©viter les doublons)
            added_count = 0
            updated_count = 0

            for movie in all_movies:
                # V√©rifier si existe d√©j√† (par titre + ann√©e)
                existing = (
                    self.db.query(LibraryItem)
                    .filter(
                        LibraryItem.title == movie.get("title"),
                        LibraryItem.year == movie.get("year"),
                        LibraryItem.media_type == MediaType.MOVIE,
                    )
                    .first()
                )

                # R√©cup√©rer le torrent_hash depuis la map
                movie_id = movie.get("id")
                torrent_hash = movie_hash_map.get(movie_id) if movie_id else None

                nb_media = 1 if movie.get("hasFile") else 0

                # R√©soudre la qualit√©: fichier t√©l√©charg√© > profil
                file_quality = movie.get("movieFile", {}).get("quality", {}).get("quality", {}).get("name")
                profile_id = movie.get("qualityProfileId")
                profile_name = quality_profiles.get(profile_id, "") if profile_id else ""
                resolved_quality = file_quality or profile_name or "Unknown"

                # Extract media_path: parent folder of the movie file
                movie_file_path = movie.get("movieFile", {}).get("path")
                media_path = str(Path(movie_file_path).parent) if movie_file_path else None

                if existing:
                    # Mettre √† jour le hash si on en a un et qu'il n'existe pas encore
                    if torrent_hash and not existing.torrent_hash:
                        existing.torrent_hash = torrent_hash
                        existing.updated_at = datetime.now(UTC)
                        updated_count += 1
                        print(f"  üîÑ Mise √† jour hash pour: {movie.get('title')} - {torrent_hash[:8]}...")
                    # Write to junction table
                    if torrent_hash:
                        self._upsert_torrent(existing.id, torrent_hash)
                    # Toujours mettre √† jour nb_media
                    existing.nb_media = nb_media
                    # Update media_path
                    if media_path:
                        existing.media_path = media_path
                    # Mettre √† jour la taille si elle √©tait √† 0
                    size_bytes = movie.get("sizeOnDisk", 0)
                    if size_bytes > 0:
                        size_gb = round(size_bytes / (1024**3), 1)
                        existing.size = f"{size_gb} GB"
                    # Mettre √† jour la qualit√© si elle √©tait un chiffre (ancien format)
                    if existing.quality and existing.quality.isdigit():
                        existing.quality = resolved_quality
                    # Repopulate added_date if missing (e.g. after migration from TEXT)
                    if existing.added_date is None:
                        raw = movie.get("added", "")
                        if raw:
                            try:
                                existing.added_date = datetime.fromisoformat(raw.replace("Z", "+00:00"))
                            except (ValueError, TypeError):
                                pass
                else:
                    # Calculer la taille
                    size_bytes = movie.get("sizeOnDisk", 0)
                    size_gb = round(size_bytes / (1024**3), 1)

                    # Date d'ajout
                    added_date = movie.get("added", "")
                    added_dt = None
                    if added_date:
                        try:
                            added_dt = datetime.fromisoformat(added_date.replace("Z", "+00:00"))
                        except (ValueError, TypeError):
                            pass

                    # R√©cup√©rer l'image
                    image_url = ""
                    for img in movie.get("images", []):
                        if img.get("coverType") == "poster":
                            image_url = img.get("remoteUrl", "")
                            break

                    if not image_url and movie.get("images"):
                        image_url = movie.get("images", [{}])[0].get("remoteUrl", "")

                    item = LibraryItem(
                        title=movie.get("title", "Unknown"),
                        year=movie.get("year", 0),
                        media_type=MediaType.MOVIE,
                        image_url=image_url,
                        image_alt=f"{movie.get('title')} poster",
                        quality=resolved_quality,
                        rating=str(movie.get("ratings", {}).get("imdb", {}).get("value", "")),
                        description=movie.get("overview", ""),
                        added_date=added_dt,
                        size=f"{size_gb} GB",
                        torrent_hash=torrent_hash,
                        nb_media=nb_media,
                        media_path=media_path,
                    )

                    self.db.add(item)
                    self.db.flush()  # Ensure item.id is available
                    added_count += 1

                    if torrent_hash:
                        self._upsert_torrent(item.id, torrent_hash)
                        print(f"  ‚úÖ {movie.get('title')} - hash: {torrent_hash[:8]}...")

            # R√©cup√©rer le calendrier (pass√© + futur)
            calendar = await connector.get_calendar(days_ahead=30, days_behind=30)

            # Ajouter/mettre √† jour le calendrier
            calendar_count = 0
            for event in calendar:
                release_date_str = event.get("physicalRelease") or event.get("digitalRelease")
                if not release_date_str:
                    continue

                try:
                    release_date = datetime.fromisoformat(release_date_str.replace("Z", "+00:00")).date()
                except (ValueError, TypeError):
                    continue

                title = event.get("title", "Unknown")

                # R√©cup√©rer l'image
                image_url = ""
                for img in event.get("images", []):
                    if img.get("coverType") == "poster":
                        image_url = img.get("remoteUrl", "")
                        break
                if not image_url and event.get("images"):
                    image_url = event.get("images", [{}])[0].get("remoteUrl", "")

                existing = (
                    self.db.query(CalendarEvent)
                    .filter(CalendarEvent.title == title, CalendarEvent.release_date == release_date)
                    .first()
                )

                if existing:
                    # Mettre √† jour les champs potentiellement vides
                    if image_url and not existing.image_url:
                        existing.image_url = image_url
                        existing.image_alt = f"{title} poster"
                else:
                    cal_event = CalendarEvent(
                        title=title,
                        media_type=MediaType.MOVIE,
                        release_date=release_date,
                        image_url=image_url,
                        image_alt=f"{title} poster",
                        status=CalendarStatus.MONITORED,
                    )
                    self.db.add(cal_event)

                calendar_count += 1

            self.db.commit()

            duration_ms = int((time.time() - start_time) * 1000)
            self.update_sync_metadata(ServiceType.RADARR, SyncStatus.SUCCESS, added_count + calendar_count, duration_ms)

            print(f"‚úÖ Radarr: {added_count} films ajout√©s, {updated_count} mis √† jour, {calendar_count} √©v√©nements")
            return {"success": True, "movies_added": added_count, "calendar_events": calendar_count}

        except Exception as e:
            duration_ms = int((time.time() - start_time) * 1000)
            self.update_sync_metadata(ServiceType.RADARR, SyncStatus.FAILED, 0, duration_ms, str(e))
            print(f"‚ùå Erreur sync Radarr: {e}")
            return {"success": False, "error": str(e)}
        finally:
            await connector.close()

    async def sync_sonarr_seasons(self) -> dict[str, Any]:
        """
        Sync seasons for TV shows from embedded series data
        Runs as part of sync_sonarr()
        """
        print("üì∫ Synchronisation des saisons Sonarr...")

        service = self.get_active_service(ServiceType.SONARR)
        if not service:
            return {"success": False, "message": "Service non configur√©"}

        connector = SonarrConnector(base_url=service.url, api_key=service.api_key, port=service.port)

        try:
            # Get all TV show items
            series_items = self.db.query(LibraryItem).filter(LibraryItem.media_type == MediaType.TV).all()

            # Get all series from Sonarr (with embedded seasons)
            series_list = await connector.get_series()

            seasons_synced = 0
            seasons_updated = 0

            for item in series_items:
                # Find Sonarr series by title + year
                series_data = next(
                    (s for s in series_list if s.get("title") == item.title and s.get("year") == item.year), None
                )

                if not series_data or "seasons" not in series_data:
                    continue

                # Upsert seasons from embedded data
                for season_data in series_data["seasons"]:
                    season_num = season_data.get("seasonNumber")
                    if season_num is None:
                        continue

                    stats = season_data.get("statistics", {})

                    existing = (
                        self.db.query(Season)
                        .filter(Season.library_item_id == item.id, Season.season_number == season_num)
                        .first()
                    )

                    if existing:
                        # Update existing season
                        existing.monitored = season_data.get("monitored", True)
                        existing.episode_count = stats.get("episodeCount", 0)
                        existing.episode_file_count = stats.get("episodeFileCount", 0)
                        existing.total_episode_count = stats.get("totalEpisodeCount", 0)
                        existing.size_on_disk = stats.get("sizeOnDisk", 0)
                        existing.statistics = stats
                        seasons_updated += 1
                    else:
                        # Create new season
                        new_season = Season(
                            library_item_id=item.id,
                            sonarr_series_id=series_data["id"],
                            season_number=season_num,
                            monitored=season_data.get("monitored", True),
                            episode_count=stats.get("episodeCount", 0),
                            episode_file_count=stats.get("episodeFileCount", 0),
                            total_episode_count=stats.get("totalEpisodeCount", 0),
                            size_on_disk=stats.get("sizeOnDisk", 0),
                            statistics=stats,
                        )
                        self.db.add(new_season)
                        seasons_synced += 1

            self.db.commit()

            print(f"‚úÖ Saisons: {seasons_synced} cr√©√©es, {seasons_updated} mises √† jour")
            return {"success": True, "seasons_synced": seasons_synced, "seasons_updated": seasons_updated}

        except Exception as e:
            print(f"‚ùå Erreur sync saisons: {e}")
            return {"success": False, "error": str(e)}
        finally:
            await connector.close()

    async def sync_sonarr(self) -> dict[str, Any]:
        """Synchroniser les donn√©es Sonarr"""
        print("üì∫ Synchronisation Sonarr...")
        start_time = time.time()

        service = self.get_active_service(ServiceType.SONARR)
        if not service:
            print("‚ö†Ô∏è Service Sonarr non configur√©")
            return {"success": False, "message": "Service non configur√©"}

        connector = SonarrConnector(base_url=service.url, api_key=service.api_key, port=service.port)

        try:
            # R√©cup√©rer toutes les s√©ries
            all_series = await connector.get_series()

            # Profils de qualit√© {id: name}
            quality_profiles = await connector.get_quality_profiles()

            # R√©cup√©rer la map seriesId -> [{hash, episode_id, season_number, is_season_pack}, ...]
            series_torrents_map = await connector.get_series_torrents_map()
            total_hashes = sum(len(v) for v in series_torrents_map.values())
            print(f"üì• {total_hashes} hash de torrents r√©cup√©r√©s depuis Sonarr ({len(series_torrents_map)} s√©ries)")
            print(f"üì∫ {len(all_series)} s√©ries trouv√©es dans Sonarr")

            added_count = 0
            updated_count = 0

            for series in all_series:
                existing = (
                    self.db.query(LibraryItem)
                    .filter(
                        LibraryItem.title == series.get("title"),
                        LibraryItem.year == series.get("year"),
                        LibraryItem.media_type == MediaType.TV,
                    )
                    .first()
                )

                # R√©cup√©rer les torrents depuis la map
                series_id = series.get("id")
                torrent_entries = series_torrents_map.get(series_id, []) if series_id else []
                first_hash = torrent_entries[0]["hash"] if torrent_entries else None

                nb_media = series.get("statistics", {}).get("episodeFileCount", 0)

                # R√©soudre la qualit√© depuis le profil (les s√©ries n'ont pas de fichier unique)
                profile_id = series.get("qualityProfileId")
                resolved_quality = quality_profiles.get(profile_id, "Unknown") if profile_id else "Unknown"

                # Extract media_path: Sonarr provides series.path directly
                media_path = series.get("path")

                if existing:
                    # Mettre √† jour le hash si on en a un et qu'il n'existe pas encore
                    if first_hash and not existing.torrent_hash:
                        existing.torrent_hash = first_hash
                        existing.updated_at = datetime.now(UTC)
                        updated_count += 1
                        print(f"  üîÑ Mise √† jour hash pour: {series.get('title')} - {first_hash[:8]}...")
                    # Upsert all torrents into junction table
                    for entry in torrent_entries:
                        self._upsert_torrent(
                            existing.id,
                            entry["hash"],
                            episode_id=entry.get("episode_id"),
                            season_number=entry.get("season_number"),
                            is_season_pack=entry.get("is_season_pack", False),
                        )
                    # Toujours mettre √† jour nb_media
                    existing.nb_media = nb_media
                    # Update media_path
                    if media_path:
                        existing.media_path = media_path
                    # Mettre √† jour la taille si elle √©tait √† 0
                    size_bytes = series.get("statistics", {}).get("sizeOnDisk", 0)
                    if size_bytes > 0:
                        size_gb = round(size_bytes / (1024**3), 1)
                        existing.size = f"{size_gb} GB"
                    # Mettre √† jour la qualit√© si elle √©tait un chiffre (ancien format)
                    if existing.quality and existing.quality.isdigit():
                        existing.quality = resolved_quality
                    # Repopulate added_date if missing (e.g. after migration from TEXT)
                    if existing.added_date is None:
                        raw = series.get("added", "")
                        if raw:
                            try:
                                existing.added_date = datetime.fromisoformat(raw.replace("Z", "+00:00"))
                            except (ValueError, TypeError):
                                pass
                else:
                    size_bytes = series.get("statistics", {}).get("sizeOnDisk", 0)
                    size_gb = round(size_bytes / (1024**3), 1)

                    added_date = series.get("added", "")
                    added_dt = None
                    if added_date:
                        try:
                            added_dt = datetime.fromisoformat(added_date.replace("Z", "+00:00"))
                        except (ValueError, TypeError):
                            pass

                    # R√©cup√©rer l'image
                    image_url = ""
                    for img in series.get("images", []):
                        if img.get("coverType") == "poster":
                            image_url = img.get("remoteUrl", "")
                            break

                    if not image_url and series.get("images"):
                        image_url = series.get("images", [{}])[0].get("remoteUrl", "")

                    item = LibraryItem(
                        title=series.get("title", "Unknown"),
                        year=series.get("year", 0),
                        media_type=MediaType.TV,
                        image_url=image_url,
                        image_alt=f"{series.get('title')} poster",
                        quality=resolved_quality,
                        rating=str(series.get("ratings", {}).get("value", "")),
                        description=series.get("overview", ""),
                        added_date=added_dt,
                        size=f"{size_gb} GB",
                        torrent_hash=first_hash,
                        nb_media=nb_media,
                        media_path=media_path,
                    )

                    self.db.add(item)
                    self.db.flush()  # Ensure item.id is available
                    added_count += 1

                    # Write all torrents to junction table
                    for entry in torrent_entries:
                        self._upsert_torrent(
                            item.id,
                            entry["hash"],
                            episode_id=entry.get("episode_id"),
                            season_number=entry.get("season_number"),
                            is_season_pack=entry.get("is_season_pack", False),
                        )

                    if first_hash:
                        print(
                            f"  ‚úÖ {series.get('title')} - {len(torrent_entries)} torrent(s), "
                            f"first: {first_hash[:8]}..."
                        )

            # R√©cup√©rer le calendrier (pass√© + futur, includeSeries=true dans le connector)
            calendar = await connector.get_calendar(days_ahead=30, days_behind=30)
            calendar_count = 0

            for event in calendar:
                if not event.get("airDate"):
                    continue

                try:
                    air_date = datetime.fromisoformat(event.get("airDate") + "T00:00:00+00:00").date()
                except (ValueError, TypeError):
                    continue

                # Titre et image depuis l'objet series (inclus via includeSeries=true)
                series_data = event.get("series", {})
                series_title = series_data.get("title") or event.get("title", "Unknown")
                season = event.get("seasonNumber", 0)
                episode_num = event.get("episodeNumber", 0)
                episode_title = event.get("title", "")
                episode_str = f"S{season:02d}E{episode_num:02d}"
                if episode_title:
                    episode_str += f" - {episode_title}"
                # Old format for backward compatibility lookup
                old_episode_str = f"Season {season}, Episode {episode_num}"

                # R√©cup√©rer l'image depuis les images de la s√©rie
                image_url = ""
                for img in series_data.get("images", []):
                    if img.get("coverType") == "poster":
                        image_url = img.get("remoteUrl", "")
                        break
                if not image_url and series_data.get("images"):
                    image_url = series_data.get("images", [{}])[0].get("remoteUrl", "")

                # Chercher un enregistrement existant (nouveau format ou ancien format)
                existing = (
                    self.db.query(CalendarEvent)
                    .filter(
                        CalendarEvent.release_date == air_date,
                        CalendarEvent.media_type == MediaType.TV,
                        CalendarEvent.episode.in_([episode_str, old_episode_str]),
                    )
                    .first()
                )

                if existing:
                    # Mettre √† jour les champs vides ou incorrects
                    if existing.title == "Unknown" and series_title != "Unknown":
                        existing.title = series_title
                    if image_url and not existing.image_url:
                        existing.image_url = image_url
                    existing.image_alt = f"{series_title} poster"
                    # Migrer l'ancien format d'√©pisode vers le nouveau
                    if existing.episode == old_episode_str:
                        existing.episode = episode_str
                else:
                    cal_event = CalendarEvent(
                        title=series_title,
                        media_type=MediaType.TV,
                        release_date=air_date,
                        episode=episode_str,
                        image_url=image_url,
                        image_alt=f"{series_title} poster",
                        status=CalendarStatus.MONITORED,
                    )
                    self.db.add(cal_event)

                calendar_count += 1

            self.db.commit()

            # Sync seasons after syncing series
            seasons_result = await self.sync_sonarr_seasons()

            duration_ms = int((time.time() - start_time) * 1000)
            self.update_sync_metadata(ServiceType.SONARR, SyncStatus.SUCCESS, added_count + calendar_count, duration_ms)

            print(
                f"‚úÖ Sonarr: {added_count} s√©ries, {calendar_count} √©v√©nements, "
                f"{seasons_result.get('seasons_synced', 0)} saisons"
            )
            return {
                "success": True,
                "series_added": added_count,
                "calendar_events": calendar_count,
                "seasons_synced": seasons_result.get("seasons_synced", 0),
            }

        except Exception as e:
            duration_ms = int((time.time() - start_time) * 1000)
            self.update_sync_metadata(ServiceType.SONARR, SyncStatus.FAILED, 0, duration_ms, str(e))
            print(f"‚ùå Erreur sync Sonarr: {e}")
            return {"success": False, "error": str(e)}
        finally:
            await connector.close()

    async def sync_sonarr_episodes(self, full_sync: bool = True, batch_size: int = 20) -> dict[str, Any]:
        """
        Sync episodes for all TV shows in batches.

        Args:
            full_sync: If True, sync all series. If False, only monitored.
            batch_size: Number of series processed per batch (default 20).
        """
        print("üì∫ Synchronisation des √©pisodes Sonarr...")
        start_time = time.time()

        service = self.get_active_service(ServiceType.SONARR)
        if not service:
            print("‚ö†Ô∏è Service Sonarr non configur√©")
            return {"success": False, "message": "Service non configur√©"}

        connector = SonarrConnector(base_url=service.url, api_key=service.api_key, port=service.port)

        try:
            # Count total series to process
            base_query = self.db.query(LibraryItem).filter(LibraryItem.media_type == MediaType.TV)
            if not full_sync:
                base_query = (
                    base_query.join(Season, Season.library_item_id == LibraryItem.id)
                    .filter(Season.monitored)
                    .distinct()
                )
            total_series = base_query.count()

            if total_series == 0:
                print("  ‚ö†Ô∏è  No series found to sync")
                return {"success": True, "series_processed": 0, "episodes_synced": 0, "episodes_updated": 0}

            print(f"  üìä {total_series} series to sync in batches of {batch_size} (full_sync={full_sync})")

            # Fetch Sonarr series list once for all batches
            series_list = await connector.get_series()
            print(f"  üì° Fetched {len(series_list)} series from Sonarr")

            episodes_synced = 0
            episodes_updated = 0
            series_processed = 0
            offset = 0

            while offset < total_series:
                batch = base_query.order_by(LibraryItem.id).offset(offset).limit(batch_size).all()
                if not batch:
                    break

                print(
                    f"  üîÑ Batch {offset // batch_size + 1}: s√©ries {offset + 1}‚Äì{offset + len(batch)}/{total_series}"
                )

                for idx, item in enumerate(batch, offset + 1):
                    print(f"  üì∫ [{idx}/{total_series}] Processing: {item.title} ({item.year})")

                    # Find matching Sonarr series
                    series_data = next(
                        (s for s in series_list if s.get("title") == item.title and s.get("year") == item.year), None
                    )

                    if not series_data:
                        print("    ‚ö†Ô∏è  Not found in Sonarr")
                        continue

                    series_id = series_data["id"]
                    print(f"    üîç Sonarr series ID: {series_id}")

                    # Fetch episodes + files
                    episodes = await connector.get_episodes_by_series(series_id)
                    episode_files = await connector.get_episode_files_by_series(series_id)
                    file_map = {f["id"]: f for f in episode_files}

                    print(f"    üì• Fetched {len(episodes)} episodes, {len(episode_files)} files")

                    # Upsert episodes
                    series_episodes_synced = 0
                    series_episodes_updated = 0

                    for ep_data in episodes:
                        season_num = ep_data.get("seasonNumber")
                        episode_num = ep_data.get("episodeNumber")

                        if season_num is None or episode_num is None:
                            continue

                        # Find or create season
                        season = (
                            self.db.query(Season)
                            .filter(Season.library_item_id == item.id, Season.season_number == season_num)
                            .first()
                        )

                        if not season:
                            season = Season(
                                library_item_id=item.id,
                                sonarr_series_id=series_id,
                                season_number=season_num,
                            )
                            self.db.add(season)
                            self.db.flush()

                        # Extract file info
                        has_file = ep_data.get("hasFile", False)
                        episode_file_id = ep_data.get("episodeFileId")
                        file_info = file_map.get(episode_file_id) if episode_file_id else None

                        # Parse air date
                        air_date = None
                        if ep_data.get("airDate"):
                            try:
                                air_date = datetime.fromisoformat(ep_data["airDate"]).date()
                            except (ValueError, TypeError):
                                pass

                        # Upsert episode
                        existing = self.db.query(Episode).filter(Episode.sonarr_episode_id == ep_data["id"]).first()

                        if existing:
                            existing.title = ep_data.get("title")
                            existing.overview = ep_data.get("overview")
                            existing.air_date = air_date
                            existing.monitored = ep_data.get("monitored", True)
                            existing.has_file = has_file
                            existing.downloaded = has_file
                            existing.sonarr_episode_file_id = episode_file_id

                            if file_info:
                                existing.file_size = file_info.get("size")
                                existing.quality_profile = file_info.get("quality", {}).get("quality", {}).get("name")
                                existing.relative_path = file_info.get("relativePath")
                                existing.episode_file_info = file_info

                            episodes_updated += 1
                            series_episodes_updated += 1
                        else:
                            new_episode = Episode(
                                season_id=season.id,
                                library_item_id=item.id,
                                sonarr_episode_id=ep_data["id"],
                                sonarr_series_id=series_id,
                                season_number=season_num,
                                episode_number=episode_num,
                                absolute_episode_number=ep_data.get("absoluteEpisodeNumber"),
                                title=ep_data.get("title"),
                                overview=ep_data.get("overview"),
                                air_date=air_date,
                                monitored=ep_data.get("monitored", True),
                                has_file=has_file,
                                downloaded=has_file,
                                sonarr_episode_file_id=episode_file_id,
                            )

                            if file_info:
                                new_episode.file_size = file_info.get("size")
                                new_episode.quality_profile = (
                                    file_info.get("quality", {}).get("quality", {}).get("name")
                                )
                                new_episode.relative_path = file_info.get("relativePath")
                                new_episode.episode_file_info = file_info

                            self.db.add(new_episode)
                            episodes_synced += 1
                            series_episodes_synced += 1

                    print(f"    ‚úÖ {series_episodes_synced} created, {series_episodes_updated} updated")
                    series_processed += 1

                # Commit after each batch to avoid large transactions
                self.db.commit()
                offset += batch_size

            duration_ms = int((time.time() - start_time) * 1000)

            print(
                f"‚úÖ √âpisodes: {episodes_synced} cr√©√©s, {episodes_updated} mis √† jour "
                f"({series_processed} s√©ries, {duration_ms}ms)"
            )
            return {
                "success": True,
                "series_processed": series_processed,
                "episodes_synced": episodes_synced,
                "episodes_updated": episodes_updated,
                "duration_ms": duration_ms,
            }

        except Exception as e:
            print(f"‚ùå Erreur sync √©pisodes: {e}")
            return {"success": False, "error": str(e)}
        finally:
            await connector.close()

    async def sync_jellyfin(self) -> dict[str, Any]:
        """Synchroniser les donn√©es Jellyfin"""
        print("üé• Synchronisation Jellyfin...")
        start_time = time.time()

        service = self.get_active_service(ServiceType.JELLYFIN)
        if not service:
            print("‚ö†Ô∏è  Service Jellyfin non configur√©")
            return {"success": False, "message": "Service non configur√©"}

        connector = JellyfinConnector(base_url=service.url, api_key=service.api_key, port=service.port)

        try:
            # R√©cup√©rer les stats
            users = await connector.get_users()
            library_stats = await connector.get_library_items()

            # R√©cup√©rer le temps de visionnage total des 30 derniers jours
            watch_time_data = await connector.get_total_watch_time(days=30)
            total_watch_hours = watch_time_data.get("total_hours", 0)

            # R√©cup√©rer les d√©tails Movies (avec dur√©e totale)
            movies_details = await connector.get_movies_details()

            # R√©cup√©rer les d√©tails TV Shows (avec dur√©e totale)
            tv_details = await connector.get_tv_shows_details()

            # Mettre √† jour les statistiques
            # Users
            user_stat = self.db.query(DashboardStatistic).filter(DashboardStatistic.stat_type == StatType.USERS).first()

            if not user_stat:
                user_stat = DashboardStatistic(stat_type=StatType.USERS)
                self.db.add(user_stat)

            user_stat.total_count = len(users)
            user_stat.details = {
                "active_users": len([u for u in users if not u.get("Policy", {}).get("IsDisabled", False)]),
                "total_watch_hours": total_watch_hours,
            }
            user_stat.last_synced = datetime.now(UTC)

            # Movies
            movie_stat = (
                self.db.query(DashboardStatistic).filter(DashboardStatistic.stat_type == StatType.MOVIES).first()
            )

            if not movie_stat:
                movie_stat = DashboardStatistic(stat_type=StatType.MOVIES)
                self.db.add(movie_stat)

            movie_stat.total_count = movies_details.get("total_movies", 0)
            movie_stat.details = {"total_hours": movies_details.get("total_hours", 0)}
            movie_stat.last_synced = datetime.now(UTC)

            # TV Shows
            tv_stat = (
                self.db.query(DashboardStatistic).filter(DashboardStatistic.stat_type == StatType.TV_SHOWS).first()
            )

            if not tv_stat:
                tv_stat = DashboardStatistic(stat_type=StatType.TV_SHOWS)
                self.db.add(tv_stat)

            tv_stat.total_count = tv_details.get("total_series", 0)
            tv_stat.details = {
                "total_series": tv_details.get("total_series", 0),
                "total_episodes": tv_details.get("total_episodes", 0),
                "total_hours": tv_details.get("total_hours", 0),
            }
            tv_stat.last_synced = datetime.now(UTC)

            self.db.commit()

            duration_ms = int((time.time() - start_time) * 1000)
            self.update_sync_metadata(ServiceType.JELLYFIN, SyncStatus.SUCCESS, len(users), duration_ms)

            movies_count = movies_details.get("total_movies", 0)
            movies_hours = movies_details.get("total_hours", 0)
            tv_count = tv_details.get("total_series", 0)
            tv_hours = tv_details.get("total_hours", 0)
            print(
                f"‚úÖ Jellyfin: {len(users)} users, {movies_count} films ({movies_hours}h), "
                f"{tv_count} s√©ries ({tv_hours}h), {total_watch_hours}h visionn√©es"
            )
            return {
                "success": True,
                "users": len(users),
                "library_stats": library_stats,
                "movies_details": movies_details,
                "tv_details": tv_details,
                "watch_hours": total_watch_hours,
            }

        except Exception as e:
            duration_ms = int((time.time() - start_time) * 1000)
            self.update_sync_metadata(ServiceType.JELLYFIN, SyncStatus.FAILED, 0, duration_ms, str(e))
            print(f"‚ùå Erreur sync Jellyfin: {e}")
            return {"success": False, "error": str(e)}
        finally:
            await connector.close()

    async def sync_jellyseerr(self) -> dict[str, Any]:
        """Synchroniser les donn√©es Jellyseerr (upsert par jellyseerr_id)"""
        print("üìù Synchronisation Jellyseerr...")
        start_time = time.time()

        service = self.get_active_service(ServiceType.JELLYSEERR)
        if not service:
            print("‚ö†Ô∏è  Service Jellyseerr non configur√©")
            return {"success": False, "message": "Service non configur√©"}

        connector = JellyseerrConnector(base_url=service.url, api_key=service.api_key, port=service.port)

        try:
            # Tester d'abord la connexion
            success, message = await connector.test_connection()
            if not success:
                raise Exception(f"Test de connexion √©chou√©: {message}")

            # R√©cup√©rer toutes les requ√™tes (tous statuts)
            requests = await connector.get_requests(limit=100, status="all")

            # Mapper les statuts Jellyseerr vers notre enum
            status_map = {
                1: RequestStatus.PENDING,
                2: RequestStatus.APPROVED,
                3: RequestStatus.DECLINED,
            }

            # Construire un set des IDs Jellyseerr re√ßus depuis l'API
            api_jellyseerr_ids = set()
            # Cache des d√©tails m√©dia pour √©viter les appels dupliqu√©s
            media_details_cache: dict[tuple[int, str], dict[str, Any]] = {}

            added_count = 0
            updated_count = 0

            for req in requests:
                try:
                    jellyseerr_id = req.get("id")
                    if not jellyseerr_id:
                        continue

                    api_jellyseerr_ids.add(jellyseerr_id)

                    media = req.get("media", {})
                    requested_by = req.get("requestedBy", {})
                    media_type_str = req.get("type", "movie")
                    tmdb_id = media.get("tmdbId")

                    # R√©cup√©rer les d√©tails du m√©dia via TMDB ID (avec cache)
                    media_details = {}
                    if tmdb_id:
                        cache_key = (tmdb_id, media_type_str)
                        if cache_key not in media_details_cache:
                            media_details_cache[cache_key] = await connector.get_media_details(tmdb_id, media_type_str)
                        media_details = media_details_cache[cache_key]

                    # Titre : depuis les d√©tails TMDB
                    title = media_details.get("title") or media_details.get("name") or "Unknown"

                    # Ann√©e : depuis releaseDate (movie) ou firstAirDate (tv)
                    year = 0
                    release_date = media_details.get("releaseDate") or media_details.get("firstAirDate") or ""
                    if release_date:
                        try:
                            year = int(release_date[:4])
                        except (ValueError, TypeError):
                            year = 0

                    # Image : posterPath depuis les d√©tails TMDB
                    poster_path = media_details.get("posterPath", "")
                    image_url = f"https://image.tmdb.org/t/p/w500{poster_path}" if poster_path else ""

                    # Description
                    description = media_details.get("overview", "")

                    # Extraction s√©curis√©e de la date de cr√©ation
                    requested_date = "Unknown"
                    if req.get("createdAt"):
                        try:
                            created_dt = datetime.fromisoformat(req.get("createdAt").replace("Z", "+00:00"))
                            requested_date = self._format_time_ago(created_dt)
                        except (ValueError, TypeError, AttributeError):
                            pass

                    req_status = status_map.get(req.get("status"), RequestStatus.PENDING)

                    # Upsert : chercher par jellyseerr_id
                    existing = (
                        self.db.query(JellyseerrRequest)
                        .filter(JellyseerrRequest.jellyseerr_id == jellyseerr_id)
                        .first()
                    )

                    if existing:
                        # Mettre √† jour les champs qui peuvent changer
                        existing.title = title
                        existing.year = year
                        existing.image_url = image_url
                        existing.image_alt = f"{title} poster"
                        existing.description = description
                        existing.status = req_status
                        existing.requested_by = requested_by.get("displayName", existing.requested_by)
                        existing.requested_by_avatar = requested_by.get("avatar")
                        existing.requested_by_user_id = requested_by.get("id")
                        existing.quality = "4K" if req.get("is4k") else "1080p"
                        existing.requested_date = requested_date
                        updated_count += 1
                    else:
                        request_item = JellyseerrRequest(
                            jellyseerr_id=jellyseerr_id,
                            title=title,
                            media_type=MediaType.MOVIE if media_type_str == "movie" else MediaType.TV,
                            year=year,
                            image_url=image_url,
                            image_alt=f"{title} poster",
                            status=req_status,
                            priority=RequestPriority.MEDIUM,
                            requested_by=requested_by.get("displayName", "Unknown"),
                            requested_by_avatar=requested_by.get("avatar"),
                            requested_by_user_id=requested_by.get("id"),
                            requested_date=requested_date,
                            quality="4K" if req.get("is4k") else "1080p",
                            description=description,
                        )
                        self.db.add(request_item)
                        added_count += 1
                except Exception as item_error:
                    print(f"‚ö†Ô∏è  Erreur traitement requ√™te Jellyseerr: {item_error}")
                    continue

            # Supprimer les requ√™tes qui n'existent plus dans l'API
            stale_deleted = 0
            if api_jellyseerr_ids:
                stale_deleted = (
                    self.db.query(JellyseerrRequest)
                    .filter(JellyseerrRequest.jellyseerr_id.notin_(api_jellyseerr_ids))
                    .delete(synchronize_session="fetch")
                )
            else:
                # Si l'API ne retourne rien, supprimer tout
                stale_deleted = self.db.query(JellyseerrRequest).delete()

            self.db.commit()

            duration_ms = int((time.time() - start_time) * 1000)
            total_synced = added_count + updated_count
            self.update_sync_metadata(ServiceType.JELLYSEERR, SyncStatus.SUCCESS, total_synced, duration_ms)

            print(f"‚úÖ Jellyseerr: {added_count} ajout√©es, {updated_count} mises √† jour, {stale_deleted} supprim√©es")
            return {
                "success": True,
                "requests_added": added_count,
                "requests_updated": updated_count,
                "requests_deleted": stale_deleted,
            }

        except Exception as e:
            duration_ms = int((time.time() - start_time) * 1000)
            self.update_sync_metadata(ServiceType.JELLYSEERR, SyncStatus.FAILED, 0, duration_ms, str(e))
            print(f"‚ùå Erreur sync Jellyseerr: {e}")
            return {"success": False, "error": str(e)}
        finally:
            await connector.close()

    async def sync_all(self) -> dict[str, Any]:
        """Synchroniser tous les services"""
        print("\n" + "=" * 50)
        print("üîÑ D√âBUT DE LA SYNCHRONISATION GLOBALE")
        print("=" * 50 + "\n")

        results = {}

        results["radarr"] = await self.sync_radarr()
        results["sonarr"] = await self.sync_sonarr()
        results["jellyfin"] = await self.sync_jellyfin()
        results["jellyseerr"] = await self.sync_jellyseerr()
        results["monitored_items"] = await self.sync_monitored_items()

        print("\n" + "=" * 50)
        print("‚úÖ SYNCHRONISATION TERMIN√âE")
        print("=" * 50 + "\n")

        return results

    def _format_time_ago(self, dt: datetime) -> str:
        """Formater une date en 'X hours ago', 'X days ago'"""
        # S'assurer que dt est timezone-aware
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=UTC)

        now = datetime.now(UTC)
        delta = now - dt

        if delta.days > 0:
            return f"{delta.days} day{'s' if delta.days > 1 else ''} ago"
        elif delta.seconds >= 3600:
            hours = delta.seconds // 3600
            return f"{hours} hour{'s' if hours > 1 else ''} ago"
        elif delta.seconds >= 60:
            minutes = delta.seconds // 60
            return f"{minutes} minute{'s' if minutes > 1 else ''} ago"
        else:
            return "just now"
